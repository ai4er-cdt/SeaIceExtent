{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5df54470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' This version of the U-Net is meant to replicate the sea ice\\n    segmentation framework as found in GitHub written in Tensorflow\\n    using PyTorch and the orginal U-Net framework.\\n    \\n    Original sea ice repo: https://github.com/asylve/Sea-Ice/blob/main/Masking-Modelling.ipynb\\n        \\n    Original U-Net described here: https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/\\n    Many of the functions have been copied and slightly modified with comments from https://github.com/milesial/Pytorch-UNet\\n    The new functions are for the Dataset class and for the create_npy_list function. \\n    \\n    The original U-Net file structure was split into multiple module files. For this working draft, \\n    all functions have been incorporated into this single file in seperate cells along with their package dependancies\\n    so that splitting into seperate modules in the future, if desired, will be simple.\\n    \\n    Additionally, this version has been slightly adapted for the single band SAR tile input compared to\\n    the original 3 band optical image (RGB) input of U-NET. \\n    \\n    Compared to the U-Net, the sea ice segmentation framework: \\n        - Does not use BatchNorm \\n        - Uses Dropout functions throughout\\n        - Concatantes the previous layer and current convoluted layer without bespoke padding\\n          and in slightly different order.\\n        - Softmax transformation in out convolutional layer.\\n        - Begins with 32 out channels rather than 64 and scales to 512 rather than >1000\\n    \\n    To adjust, must change: dir_image, dir_checkpoint, n_channels, n_classes and bilinear variables.\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" PLEASE DO NOT DIRECTLY EDIT THIS FILE.\n",
    "\n",
    "    This version of the U-Net is meant to replicate the sea ice\n",
    "    segmentation framework as found in GitHub written in Tensorflow\n",
    "    using PyTorch and the orginal U-Net framework.\n",
    "    \n",
    "    Original sea ice repo: https://github.com/asylve/Sea-Ice/blob/main/Masking-Modelling.ipynb\n",
    "        \n",
    "    Original U-Net described here: https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/\n",
    "    Many of the functions have been copied and slightly modified with comments from https://github.com/milesial/Pytorch-UNet\n",
    "    The new functions are for the Dataset class and for the create_npy_list function. \n",
    "    \n",
    "    The original U-Net file structure was split into multiple module files. For this working draft, \n",
    "    all functions have been incorporated into this single file in seperate cells along with their package dependancies\n",
    "    so that splitting into seperate modules in the future, if desired, will be simple.\n",
    "    \n",
    "    Additionally, this version has been slightly adapted for the single band SAR tile input compared to\n",
    "    the original 3 band optical image (RGB) input of U-NET. \n",
    "    \n",
    "    Compared to the U-Net, the sea ice segmentation framework: \n",
    "        - Does not use BatchNorm \n",
    "        - Uses Dropout functions throughout\n",
    "        - Concatantes the previous layer and current convoluted layer without bespoke padding\n",
    "          and in slightly different order.\n",
    "        - Softmax transformation in out convolutional layer.\n",
    "        - Begins with 32 out channels rather than 64 and scales to 512 rather than >1000\n",
    "    \n",
    "    To adjust, must change: dir_image, dir_checkpoint, n_channels, n_classes and bilinear variables.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30dba784",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Parts of the U-Net model \"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            #nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            #nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool, dropout then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(p=0.5, inplace=True),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling, single conv, concatination, dropout, then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            #nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.dconv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        self.dropout = nn.Dropout(p=0.5, inplace=True)\n",
    "            \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        x1 = self.conv(x1)\n",
    "        \n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        #print(x1.size(), x2.size(), x.size())\n",
    "        x = self.dropout(x)\n",
    "        return self.dconv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.outconv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1),\n",
    "            nn.Softmax() #softmax converts the output to a list of probabilities that must sum to 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.outconv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a2b640f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Full assembly of the parts to form the complete network \"\"\"\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 32)\n",
    "        self.down1 = Down(32, 64)\n",
    "        self.down2 = Down(64, 128)\n",
    "        self.down3 = Down(128, 256)\n",
    "        self.down4 = Down(256, 512)\n",
    "        self.up1 = Up(512, 256, bilinear)\n",
    "        self.up2 = Up(256, 128, bilinear)\n",
    "        self.up3 = Up(128, 64, bilinear)\n",
    "        self.up4 = Up(64, 32, bilinear)\n",
    "        self.outc = OutConv(32, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0aaaab1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" CNN Dataset preparation functions \"\"\"\n",
    "\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset  # For custom data-sets\n",
    "from torchvision import transforms\n",
    "import glob\n",
    "import numpy as np\n",
    "# From https://discuss.pytorch.org/t/beginner-how-do-i-write-a-custom-dataset-that-allows-me-to-return-image-and-its-target-image-not-label-as-a-pair/13988/4\n",
    "# And https://discuss.pytorch.org/t/how-make-customised-dataset-for-semantic-segmentation/30881\n",
    "\n",
    "\n",
    "def create_npy_list(image_directory, img_string=\"sar\"):\n",
    "    \"\"\"A function that returns a list of the names of the SAR/MODIS and labelled .npy files in a directory. These lists can\n",
    "    then be used as an argument for the Dataset class instantiation. The function also checks that the specified directory \n",
    "    contains matching sar or MODIS/labelled pairs -- specifically, a label.npy file for each image file.\"\"\"\n",
    "\n",
    "    img_names = sorted(glob.glob(str(image_directory) + '/*_' + img_string + '.npy'))\n",
    "    label_names = sorted(glob.glob(str(image_directory) + '/*_labels.npy'))\n",
    "\n",
    "    # In-depth file-by-file check for matching sar-label pairs in the directory -- assuming  each sar image has a corresponding\n",
    "    # labeled image.\n",
    "    img_label_pairs = []\n",
    "    for image in img_names:\n",
    "        expected_label_name = image.replace(img_string, \"labels\")\n",
    "        if expected_label_name in label_names:\n",
    "            img_label_pairs.append((image, expected_label_name))\n",
    "        else:\n",
    "            raise Exception(f'{img_string} tile name {image} does not have a matching labeled tile.')\n",
    "            \n",
    "    return img_label_pairs\n",
    "\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    \"\"\"GTC Code for a dataset class. The class is instantiated with list of filenames within a directory (created using\n",
    "    the list_npy_filenames function). The __getitem__ method pairs up corresponding sar-label .npy file pairs. This\n",
    "    dataset can then be input to a dataloader.\"\"\"\n",
    "    \n",
    "    def __init__(self, paths, isSingleBand = True):\n",
    "        self.paths = paths\n",
    "        self.isSingleBand = isSingleBand\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image = torch.from_numpy(np.vstack(np.load(self.paths[index][0])).astype(float))\n",
    "        if self.isSingleBand:\n",
    "            image = image[None,:]\n",
    "        else:\n",
    "            image = torch.permute(image, (2,0,1))\n",
    "        mask_raw = (np.load(self.paths[index][1]))\n",
    "        maskremap100 = np.where(mask_raw == 100, 0, mask_raw)\n",
    "        maskremap200 = np.where(maskremap100 == 200, 1, maskremap100)\n",
    "        mask = torch.from_numpy(np.vstack(maskremap200).astype(float))\n",
    "        \n",
    "        #assert image.size == mask.size, \\\n",
    "        #    'Image and mask {name} should be the same size, but are {img.size} and {mask.size}'\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'mask': mask\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths) \n",
    "    \n",
    "\n",
    "class CustomImageAugmentDataset(Dataset):\n",
    "    \"\"\"GTC Code for an augmented dataset class. The class is instantiated with a list of filenames within a directory \n",
    "    (created using the list_npy_filenames function). The __getitem__ method pairs up corresponding image-label .npy file\n",
    "    pairs. If specified, augmentations are also applied to the images with a probability. There is a ~25% chance that \n",
    "    no augmentations are applied and a 20% chance of each of the following augmentations: horizontal flip, vertical \n",
    "    flip, 90 degree rotation (anti-clockwise & clockwise), 180 degree rotation, random crop. Multiple augmentations are\n",
    "    applied in sequence. This dataset can then be input to a dataloader.\"\"\"\n",
    "\n",
    "    def __init__(self, paths, augmentation):\n",
    "        self.paths = paths\n",
    "        self.augmentation = augmentation\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = torch.from_numpy(np.vstack(np.load(self.paths[index][0])).astype(float))[None, :]\n",
    "        mask_raw = (np.load(self.paths[index][1]))\n",
    "        maskremap100 = np.where(mask_raw == 100, 100, mask_raw) # 0\n",
    "        maskremap200 = np.where(maskremap100 == 200, 200, maskremap100) # 1\n",
    "        mask = torch.from_numpy(np.vstack(maskremap200).astype(float))\n",
    "\n",
    "        if self.augmentation:\n",
    "            image_pair = self.augment_image(image, mask)\n",
    "        else:\n",
    "            image_pair = {'image': image, 'mask': mask}\n",
    "\n",
    "        return image_pair\n",
    "\n",
    "    @staticmethod\n",
    "    def augment_image(original_image, original_mask):\n",
    "\n",
    "        augmented_image = original_image\n",
    "        augmented_mask = original_mask\n",
    "\n",
    "        aug_probabilities = np.random.choice(a=[0,1], size=6, p=[0.8, 0.2])\n",
    "\n",
    "        if aug_probabilities[0]: # Horizontal flip\n",
    "            augment_function = transforms.RandomHorizontalFlip(p=1)\n",
    "            augmented_image, augmented_mask = augment_function(augmented_image), augment_function(augmented_mask)\n",
    "\n",
    "        if aug_probabilities[1]: # Horizontal flip\n",
    "            augment_function = transforms.RandomVerticalFlip(p=1)\n",
    "            augmented_image, augmented_mask = augment_function(augmented_image), augment_function(augmented_mask)\n",
    "\n",
    "        if aug_probabilities[2]: # 90 degree rotation anti-clockwise\n",
    "            augmented_image = torch.rot90(augmented_image, k=1, dims=[1, 2])\n",
    "            augmented_mask = torch.rot90(augmented_mask, k=1, dims=[0, 1])\n",
    "\n",
    "        if aug_probabilities[3]: # 180 degree rotation\n",
    "            augmented_image = torch.rot90(augmented_image, k=2, dims=[1, 2])\n",
    "            augmented_mask = torch.rot90(augmented_mask, k=2, dims=[0, 1])\n",
    "\n",
    "        if aug_probabilities[4]: # 90 degree rotation clockwise\n",
    "            augmented_image = torch.rot90(augmented_image, k=-1, dims=[1, 2])\n",
    "            augmented_mask = torch.rot90(augmented_mask, k=-1, dims=[0, 1])\n",
    "\n",
    "        if aug_probabilities[5]: # Random crop (and resize)\n",
    "            augment_function = transforms.Compose([transforms.RandomCrop(size=256),\n",
    "                                                   transforms.Resize(256)])\n",
    "            augmented_image, augmented_mask = augment_function(augmented_image), augment_function(augmented_mask)\n",
    "\n",
    "        \"\"\"\n",
    "        if aug_probabilities[6]: # Gaussian blur\n",
    "            augment_function = transforms.GaussianBlur(kernel_size=(7,13), sigma=(0.1, 0.2))\n",
    "            augmented_image, augmented_mask = augment_function(augmented_image), augment_function(augmented_mask)\n",
    "        \"\"\"\n",
    "        return {'image': augmented_image, 'mask': augmented_mask}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fb2aecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "dir_img = Path('/mnt/g/Shared drives/2021-gtc-sea-ice/trainingdata/tiled/')\n",
    "img_names = sorted(glob.glob(str(dir_img) + '/*_modis.npy'))\n",
    "img_list = create_npy_list(dir_img)\n",
    "dataset = Custom(img_list, True)\n",
    "#len(dataset)\n",
    "len(img_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d202f69",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: '/mnt/g'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_25935/262984642.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/mnt/g/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m: [Errno 22] Invalid argument: '/mnt/g'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "\n",
    "print (([name for name in os.listdir(Path('/mnt/g/')) ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e607a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Dice Loss scoring functions \"\"\"\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "def dice_coeff(input: Tensor, target: Tensor, reduce_batch_first: bool = False, epsilon=1e-6):\n",
    "    # Average of Dice coefficient for all batches, or for a single mask\n",
    "    assert input.size() == target.size()\n",
    "    if input.dim() == 2 and reduce_batch_first:\n",
    "        raise ValueError(f'Dice: asked to reduce batch but got tensor without batch dimension (shape {input.shape})')\n",
    "\n",
    "    if input.dim() == 2 or reduce_batch_first:\n",
    "        inter = torch.dot(input.reshape(-1), target.reshape(-1))\n",
    "        sets_sum = torch.sum(input) + torch.sum(target)\n",
    "        if sets_sum.item() == 0:\n",
    "            sets_sum = 2 * inter\n",
    "\n",
    "        return (2 * inter + epsilon) / (sets_sum + epsilon)\n",
    "    else:\n",
    "        # compute and average metric for each batch element\n",
    "        dice = 0\n",
    "        for i in range(input.shape[0]):\n",
    "            dice += dice_coeff(input[i, ...], target[i, ...])\n",
    "        return dice / input.shape[0]\n",
    "\n",
    "\n",
    "def multiclass_dice_coeff(input: Tensor, target: Tensor, reduce_batch_first: bool = False, epsilon=1e-6):\n",
    "    # Average of Dice coefficient for all classes\n",
    "    assert input.size() == target.size()\n",
    "    dice = 0\n",
    "    for channel in range(input.shape[1]):\n",
    "        dice += dice_coeff(input[:, channel, ...], target[:, channel, ...], reduce_batch_first, epsilon)\n",
    "\n",
    "    return dice / input.shape[1]\n",
    "\n",
    "\n",
    "def dice_loss(input: Tensor, target: Tensor, multiclass: bool = False):\n",
    "    # Dice loss (objective to minimize) between 0 and 1\n",
    "    assert input.size() == target.size()\n",
    "    fn = multiclass_dice_coeff if multiclass else dice_coeff\n",
    "    return 1 - fn(input, target, reduce_batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "162a449f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Model evaluation functions \"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def evaluate(net, dataloader, device):\n",
    "    net.eval()\n",
    "    num_val_batches = len(dataloader)\n",
    "    dice_score = 0\n",
    "\n",
    "    # iterate over the validation set\n",
    "    for batch in tqdm(dataloader, total=num_val_batches, desc='Validation round', unit='batch', leave=False):\n",
    "        image, mask_true = batch['image'], batch['mask']\n",
    "        # move images and labels to correct device and type\n",
    "        image = image.to(device=device, dtype=torch.float32)\n",
    "        mask_true = mask_true.to(device=device, dtype=torch.long)\n",
    "        mask_true = F.one_hot(mask_true, net.n_classes).permute(0, 3, 1, 2).float()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # predict the mask\n",
    "            mask_pred = net(image)\n",
    "\n",
    "            # convert to one-hot format\n",
    "            if net.n_classes == 1:\n",
    "                mask_pred = (F.sigmoid(mask_pred) > 0.5).float()\n",
    "                # compute the Dice score\n",
    "                dice_score += dice_coeff(mask_pred, mask_true, reduce_batch_first=False)\n",
    "            else:\n",
    "                mask_pred = F.one_hot(mask_pred.argmax(dim=1), net.n_classes).permute(0, 3, 1, 2).float()\n",
    "                # compute the Dice score, ignoring background\n",
    "                dice_score += multiclass_dice_coeff(mask_pred[:, 1:, ...], mask_true[:, 1:, ...], reduce_batch_first=False)\n",
    "\n",
    "           \n",
    "\n",
    "    net.train()\n",
    "\n",
    "    # Fixes a potential division by zero error\n",
    "    if num_val_batches == 0:\n",
    "        return dice_score\n",
    "    return dice_score / num_val_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfefa63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmlisaius\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.10 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/mlisaius/my-test-project/runs/ak7uj2kk\" target=\"_blank\">fine-voice-1</a></strong> to <a href=\"https://wandb.ai/mlisaius/my-test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using device cpu\n",
      "INFO: Network:\n",
      "\t1 input channels\n",
      "\t2 output channels (classes)\n",
      "\tBilinear upscaling\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_25747/144482909.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    188\u001b[0m                   \u001b[0mimg_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m                   \u001b[0mval_percent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m                   amp=args.amp)\n\u001b[0m\u001b[1;32m    191\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'INTERRUPTED.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_25747/144482909.py\u001b[0m in \u001b[0;36mtrain_net\u001b[0;34m(net, device, epochs, batch_size, learning_rate, val_percent, save_checkpoint, img_scale, amp)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# 3. Create data loaders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mloader_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mloader_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mval_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mloader_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/unetenv/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers)\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# map-style\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/unetenv/lib/python3.7/site-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             raise ValueError(\"num_samples should be a positive integer \"\n\u001b[0;32m--> 103\u001b[0;31m                              \"value, but got num_samples={}\".format(self.num_samples))\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "\"\"\" Implementation of model \"\"\"\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "wandb.init(project=\"my-test-project\", entity=\"mlisaius\")\n",
    "\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "dir_img = Path('/mnt/g/Shared drives/2021-gtc-sea-ice/trainingdata/tiled/')\n",
    "dir_checkpoint = Path('/mnt/g/Shared drives/2021-gtc-sea-ice/model/checkpoints/')\n",
    "\n",
    "\n",
    "def train_net(net,\n",
    "              device,\n",
    "              epochs: int = 5,\n",
    "              batch_size: int = 10,\n",
    "              learning_rate: float = 0.001,\n",
    "              val_percent: float = 0.1,\n",
    "              save_checkpoint: bool = True,\n",
    "              img_scale: float = 0.5,\n",
    "              amp: bool = False):\n",
    "    # 1. Create dataset\n",
    "    img_list = create_npy_list(dir_img)\n",
    "    dataset = CustomImageDataset(img_list, True)\n",
    "    \n",
    "    # 2. Split into train / validation partitions\n",
    "    n_val = int(len(dataset) * val_percent)\n",
    "    n_train = len(dataset) - n_val\n",
    "    train_set, val_set = random_split(dataset, [n_train, n_val], generator=torch.Generator().manual_seed(0))\n",
    "    \n",
    "    # 3. Create data loaders\n",
    "    loader_args = dict(batch_size=batch_size, num_workers=4, pin_memory=True)\n",
    "    train_loader = DataLoader(train_set, shuffle=True, **loader_args)\n",
    "    val_loader = DataLoader(val_set, shuffle=False, drop_last=True, **loader_args)\n",
    "    \n",
    "    # Initialize logging\n",
    "    experiment = wandb.init(project='U-Net', resume='allow', anonymous='must')\n",
    "    experiment.config.update(dict(epochs=epochs, batch_size=batch_size, learning_rate=learning_rate,\n",
    "                                  val_percent=val_percent, save_checkpoint=save_checkpoint, img_scale=img_scale,\n",
    "                                  amp=amp))\n",
    "\n",
    "    logging.info(f'''Starting training:\n",
    "        Epochs:          {epochs}\n",
    "        Batch size:      {batch_size}\n",
    "        Learning rate:   {learning_rate}\n",
    "        Training size:   {n_train}\n",
    "        Validation size: {n_val}\n",
    "        Checkpoints:     {save_checkpoint}\n",
    "        Device:          {device.type}\n",
    "        Images scaling:  {img_scale}\n",
    "        Mixed Precision: {amp}\n",
    "    ''')\n",
    "\n",
    "    # 4. Set up the optimizer, the loss, the learning rate scheduler and the loss scaling for AMP\n",
    "    optimizer = optim.RMSprop(net.parameters(), lr=learning_rate, weight_decay=1e-8, momentum=0.9)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=2)  # goal: maximize Dice score\n",
    "    grad_scaler = torch.cuda.amp.GradScaler(enabled=amp)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    global_step = 0\n",
    "    \n",
    "    # 5. Begin training\n",
    "    for epoch in range(epochs):\n",
    "        net.train()\n",
    "        epoch_loss = 0\n",
    "        with tqdm(total=n_train, desc=f'Epoch {epoch + 1}/{epochs}', unit='img') as pbar:\n",
    "            for batch in train_loader:\n",
    "                images = batch['image']\n",
    "                true_masks = batch['mask']\n",
    "                \n",
    "                assert images.shape[1] == net.n_channels, \\\n",
    "                    f'Network has been defined with {net.n_channels} input channels, ' \\\n",
    "                    f'but loaded images have {images.shape[1]} channels of {images.shape}. Please check that ' \\\n",
    "                    'the images are loaded correctly.'\n",
    "\n",
    "                images = images.to(device=device, dtype=torch.float32)\n",
    "                true_masks = true_masks.to(device=device, dtype=torch.long)\n",
    "\n",
    "                with torch.cuda.amp.autocast(enabled=amp):\n",
    "                    masks_pred = net(images)\n",
    "                    loss = criterion(masks_pred, true_masks) \\\n",
    "                           + dice_loss(F.softmax(masks_pred, dim=1).float(),\n",
    "                                       F.one_hot(true_masks, net.n_classes).permute(0, 3, 1, 2).float(),\n",
    "                                       multiclass=True)\n",
    "                # change number for permute?\n",
    "                \n",
    "                print(net.parameters())\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                grad_scaler.scale(loss).backward()\n",
    "                grad_scaler.step(optimizer)\n",
    "                grad_scaler.update()\n",
    "\n",
    "                pbar.update(images.shape[0])\n",
    "                global_step += 1\n",
    "                epoch_loss += loss.item()\n",
    "                experiment.log({\n",
    "                    'train loss': loss.item(),\n",
    "                    'step': global_step,\n",
    "                    'epoch': epoch\n",
    "                })\n",
    "                pbar.set_postfix(**{'loss (batch)': loss.item()})\n",
    "\n",
    "                # Evaluation round\n",
    "                division_step = (n_train // (10 * batch_size))\n",
    "                if division_step > 0:\n",
    "                    if global_step % division_step == 0:\n",
    "                        histograms = {}\n",
    "                        for tag, value in net.named_parameters():\n",
    "                            tag = tag.replace('/', '.')\n",
    "                            histograms['Weights/' + tag] = wandb.Histogram(value.data.cpu())\n",
    "                            histograms['Gradients/' + tag] = wandb.Histogram(value.grad.data.cpu())\n",
    "\n",
    "                        val_score = evaluate(net, val_loader, device)\n",
    "                        scheduler.step(val_score)\n",
    "\n",
    "                        logging.info('Validation Dice score: {}'.format(val_score))\n",
    "                        experiment.log({\n",
    "                            'learning rate': optimizer.param_groups[0]['lr'],\n",
    "                            'validation Dice': val_score,\n",
    "                            'images': wandb.Image(images[0].cpu()),\n",
    "                            'masks': {\n",
    "                                'true': wandb.Image(true_masks[0].float().cpu()),\n",
    "                                'pred': wandb.Image(torch.softmax(masks_pred, dim=1).argmax(dim=1)[0].float().cpu()),\n",
    "                            },\n",
    "                            'step': global_step,\n",
    "                            'epoch': epoch,\n",
    "                            **histograms\n",
    "                        })\n",
    "\n",
    "        if save_checkpoint:\n",
    "            Path(dir_checkpoint).mkdir(parents=True, exist_ok=True)\n",
    "            torch.save(net.state_dict(), str(dir_checkpoint / 'checkpoint_epoch{}.pth'.format(epoch + 1)))\n",
    "            logging.info(f'Checkpoint {epoch + 1} saved!')\n",
    "\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(description='Train the UNet on images and target masks')\n",
    "    parser.add_argument('--epochs', '-e', metavar='E', type=int, default=5, help='Number of epochs')\n",
    "    parser.add_argument('--batch-size', '-b', dest='batch_size', metavar='B', type=int, default=1, help='Batch size')\n",
    "    parser.add_argument('--learning-rate', '-l', metavar='LR', type=float, default=0.00001,\n",
    "                        help='Learning rate', dest='lr')\n",
    "    parser.add_argument('--load', '-f', type=str, default=False, help='Load model from a .pth file')\n",
    "    parser.add_argument('--scale', '-s', type=float, default=0.5, help='Downscaling factor of the images')\n",
    "    parser.add_argument('--validation', '-v', dest='val', type=float, default=10.0,\n",
    "                        help='Percent of the data that is used as validation (0-100)')\n",
    "    parser.add_argument('--amp', action='store_true', default=False, help='Use mixed precision')\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = get_args()\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    logging.info(f'Using device {device}')\n",
    "\n",
    "    # Change here to adapt to your data\n",
    "    # n_channels=3 for RGB images\n",
    "    # n_classes is the number of probabilities you want to get per pixel\n",
    "    net = UNet(n_channels=1, n_classes=2, bilinear=True)\n",
    "\n",
    "    logging.info(f'Network:\\n'\n",
    "                 f'\\t{net.n_channels} input channels\\n'\n",
    "                 f'\\t{net.n_classes} output channels (classes)\\n'\n",
    "                 f'\\t{\"Bilinear\" if net.bilinear else \"Transposed conv\"} upscaling')\n",
    "\n",
    "#    if args.load:\n",
    "#        net.load_state_dict(torch.load(args.load, map_location=device))\n",
    "#        logging.info(f'Model loaded from {args.load}')\n",
    "\n",
    "    net.to(device=device)\n",
    "    try:\n",
    "        train_net(net=net,\n",
    "                  epochs=args.epochs,\n",
    "                  batch_size=args.batch_size,\n",
    "                  learning_rate=args.lr,\n",
    "                  device=device,\n",
    "                  img_scale=args.scale,\n",
    "                  val_percent=args.val / 100,\n",
    "                  amp=args.amp)\n",
    "    except KeyboardInterrupt:\n",
    "        torch.save(net.state_dict(), 'INTERRUPTED.pth')\n",
    "        logging.info('Saved interrupt')\n",
    "        sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d1dcb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
