{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98a6d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: gc2ojpg4\n",
      "Sweep URL: https://wandb.ai/mlisaius/hyp-sweep-jasmin-mcl/sweeps/gc2ojpg4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xpeusitk with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_name: dpn68\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_scale: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.07755600231714047\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmomentum: 0.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimiser: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tvalidation_percent: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.009709873335638986\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/mlisaius/hyp-sweep-jasmin-mcl/runs/xpeusitk\" target=\"_blank\">cosmic-sweep-1</a></strong> to <a href=\"https://wandb.ai/mlisaius/hyp-sweep-jasmin-mcl\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "Sweep page: <a href=\"https://wandb.ai/mlisaius/hyp-sweep-jasmin-mcl/sweeps/gc2ojpg4\" target=\"_blank\">https://wandb.ai/mlisaius/hyp-sweep-jasmin-mcl/sweeps/gc2ojpg4</a><br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_bytes 524408\n",
      "num_bytes 1048816\n",
      "num_bytes 1573224\n",
      "num_bytes 2097632\n",
      "num_bytes 2622040\n",
      "num_bytes 3146448\n",
      "num_bytes 3670856\n",
      "num_bytes 4195264\n",
      "num_bytes 4719672\n",
      "num_bytes 5244080\n",
      "num_bytes 5768488\n",
      "num_bytes 6292896\n",
      "num_bytes 6817304\n",
      "num_bytes 7341712\n",
      "num_bytes 7866120\n",
      "num_bytes 8390528\n",
      "num_bytes 8914936\n",
      "num_bytes 9439344\n",
      "num_bytes 9963752\n",
      "num_bytes 10488160\n",
      "10488160 bytes in small set\n",
      "20 tiles in set\n",
      "full set num tiles divided by small set num tiles = 278.2\n",
      "train:   0%|                                                                                      | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "\"\"\"This script runs best if you login to wandb through the terminal before running the script using: $wandb login\"\"\"\n",
    "\n",
    "try:\n",
    "    from dataset_preparation import *\n",
    "    from evaluation import *\n",
    "    from network_structure import *\n",
    "except:\n",
    "    from unet.dataset_preparation import *\n",
    "    from unet.evaluation import *\n",
    "    from unet.network_structure import *\n",
    "\n",
    "import wandb\n",
    "import torch.optim as optim\n",
    "from pathlib import Path\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def create_checkpoint_dir(parent_folder_checkpoint, img_type, model_type,\n",
    "                         encoder, optimiser, lr, batchsize, weightdecay):\n",
    "    now = datetime.now()\n",
    "    dt_string = now.strftime(\"%d%m%Y_%H%M%S\")\n",
    "    dir_list = os.listdir(parent_folder_checkpoint)\n",
    "    FOLDER_EXISTS = True\n",
    "    x = 1 \n",
    "    while FOLDER_EXISTS: \n",
    "        dir_checkpoint = str('{}_{}_encode{}_optm{}_batchsize{}_learnrate{}_weightdecay{}_date{}_{}'.format(model_type, img_type, \n",
    "                                                   str(encoder), str(optimiser), str(batchsize), str(lr), \n",
    "                                                   str(weightdecay), str(dt_string), str(x)))\n",
    "        path = os.path.join(parent_folder_checkpoint, Path(dir_checkpoint))\n",
    "        if dir_checkpoint in dir_list:\n",
    "            x += 1\n",
    "        else:\n",
    "            FOLDER_EXISTS = False\n",
    "            os.mkdir(path)\n",
    "            return(path)\n",
    "        \n",
    "\n",
    "def build_optimiser(network, config):\n",
    "    \"\"\"Builds the training optimiser based on the parametersd in the config file\"\"\"\n",
    "    if config.optimiser == \"sgd\":\n",
    "        optimiser = optim.SGD(network.parameters(),\n",
    "                              config.learning_rate, config.momentum)\n",
    "    elif config.optimiser == \"adam\":\n",
    "        optimiser = optim.Adam(network.parameters(),\n",
    "                               lr=config.learning_rate)\n",
    "    return optimiser\n",
    "\n",
    "\n",
    "def train_and_validate(config=None, amp=False, device=torch.device('cuda')):\n",
    "\n",
    "    # Inputs for the helper functions\n",
    "    # Image and model params\n",
    "    img_dir = '/home/users/mcl66/SeaIce/tiled512/'\n",
    "    #img_dir = '/mnt/g/Shared drives/2021-gtc-sea-ice/trainingdata/tiled512/train/'\n",
    "    image_type = 'sar'\n",
    "    model_type = \"pretrained\"\n",
    "    workers = 10\n",
    "    \n",
    "    # for checkpoint saving\n",
    "    folder_checkpoint = '/home/users/mcl66/SeaIce/checkpoints/'\n",
    "    save_checkpoint = True\n",
    "    \n",
    "    # unlikely to need modification\n",
    "    return_type = 'values'\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    max_score = 0\n",
    "    \n",
    "    if image_type == \"sar\":\n",
    "        is_single_band = True\n",
    "        n_channels = 1\n",
    "    elif image_type == \"modis\":\n",
    "        is_single_band = False\n",
    "        n_channels = 3\n",
    "\n",
    "    # Initialize a new wandb run\n",
    "    with wandb.init(config=config):\n",
    "        # If called by wandb.agent, as below,\n",
    "        # this config will be set by Sweep Controller\n",
    "        config = wandb.config\n",
    "        \n",
    "        \n",
    "        net = smp.Unet(encoder_name=config.encoder_name, encoder_weights='imagenet', decoder_use_batchnorm=True,\n",
    "                 decoder_attention_type=None, in_channels=n_channels, classes=1, encoder_depth=5)\n",
    "        net = net.double()\n",
    "\n",
    "        # Loader\n",
    "        img_list = create_npy_list(img_dir, image_type)\n",
    "        img_list = small_sample(img_list)\n",
    "\n",
    "        train_img_list, val_img_list, n_train, n_val = split_img_list(img_list, config.validation_percent)\n",
    "\n",
    "        train_dataset = CustomImageAugmentDataset(train_img_list, is_single_band, return_type, True)\n",
    "        validation_dataset = CustomImageAugmentDataset(val_img_list, is_single_band, return_type, False)\n",
    "\n",
    "        train_loader, val_loader = create_dataloaders(train_dataset, validation_dataset, config.batch_size, workers)\n",
    "\n",
    "        # Optimiser\n",
    "        optimiser = build_optimiser(net, config)\n",
    "\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimiser, 'max',\n",
    "                                                         patience=2)  # CHECK goal: maximize Dice score\n",
    "        grad_scaler = torch.cuda.amp.GradScaler(enabled=False)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        global_step = 0\n",
    "        \n",
    "        loss = smp.utils.losses.DiceLoss()\n",
    "        metrics = [smp.utils.metrics.IoU(threshold=0.5),]\n",
    "        \n",
    "        train_epoch = smp.utils.train.TrainEpoch(\n",
    "            net,\n",
    "            loss=loss,\n",
    "            metrics=metrics,\n",
    "            optimizer=optimiser,\n",
    "            verbose=True,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        valid_epoch = smp.utils.train.ValidEpoch(\n",
    "            net,\n",
    "            loss=loss,\n",
    "            metrics=metrics,\n",
    "            verbose=True,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "\n",
    "        # Begin training\n",
    "        run_loss_train = 0\n",
    "        run_loss_val = 0\n",
    "        dir_checkpoint = ''\n",
    "        for epoch in range(config.epochs):\n",
    "            train_logs = train_epoch.run(train_loader)\n",
    "            valid_logs = valid_epoch.run(val_loader)\n",
    "\n",
    "            # do something (save model, change lr, etc.)\n",
    "            #if max_score < valid_logs['iou_score']:\n",
    "            #    max_score = valid_logs['iou_score']\n",
    "            if save_checkpoint:\n",
    "                if epoch == 1:\n",
    "                    dir_checkpoint = create_checkpoint_dir(folder_checkpoint, image_type, model_type, config.encoder_name, config.optimiser, \n",
    "                                                           config.learning_rate, config.batch_size, config.weight_decay)\n",
    "                #torch.save(net.state_dict(), str(dir_checkpoint / 'checkpoint_epoch{}.pth'.format(epoch + 1)))\n",
    "                torch.save(net.state_dict(), dir_checkpoint+f'checkpoint_epoch{epoch + 1}.pth')\n",
    "                \n",
    "            # Optimisation\n",
    "            optimiser.zero_grad(set_to_none=True)\n",
    "            grad_scaler.scale(loss).backward()\n",
    "            grad_scaler.step(optimiser)\n",
    "            grad_scaler.update()\n",
    "\n",
    "            #pbar.update(images.shape[0])\n",
    "            global_step += 1\n",
    "            batch_loss = loss.item()\n",
    "            epoch_loss += batch_loss\n",
    "            wandb.log({\"Batch Loss, Training\": batch_loss}, step=global_step)\n",
    "            #pbar.set_postfix(**{'loss (batch)': loss.item()})\n",
    "            n_batches += 1\n",
    "\n",
    "            val_score = evaluate(net, val_loader, device, epsilon=config.weight_decay)\n",
    "            #print('6')\n",
    "            print(f'\\nVal Score: {val_score}, Epoch: {epoch}')\n",
    "\n",
    "            #wandb.log({\"Batch Loss, Validation\": val_score_list[index]}, step=global_step-(len(val_score_list)+index))\n",
    "            #wandb.log({\"Epoch Validation Loss\": val_score}) , step=global_step-(len(val_score_list)+index)\n",
    "            scheduler.step(val_score)\n",
    "\n",
    "            avg_epoch_training_loss = epoch_loss / n_batches\n",
    "            #wandb.log({\"Epoch Training Loss\": avg_epoch_training_loss})\n",
    "            run_loss_train += avg_epoch_training_loss\n",
    "            #run_loss_val += val_score\n",
    "            print('Logging Epoch Scores')\n",
    "            wandb.log({\"Epoch Loss, Training\": avg_epoch_training_loss, \"Epoch Loss, Validation\": val_score, \"Epoch\": epoch+1}, step=global_step)\n",
    "           # if save_checkpoint:\n",
    "           #     if epoch == 0:\n",
    "           #         dir_checkpoint = create_checkpoint_dir(folder_checkpoint, image_type, model_type, config.encoder_name, config.optimiser, \n",
    "           #                                                config.learning_rate, config.batch_size, config.weight_decay)\n",
    "           #     torch.save(net.state_dict(), str(dir_checkpoint / 'checkpoint_epoch{}.pth'.format(epoch + 1)))\n",
    "        wandb.log({\"Run Loss, Training\": run_loss_train / config.epochs, \"Run Loss, Validation\": val_score}, step=global_step)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    #wandb.init(project=\"test-hyptuning\")\n",
    "\n",
    "    # Configuring wandb settings\n",
    "    sweep_config = {\n",
    "        'method': 'random',  # Random search method -- less computationally expensive yet effective.\n",
    "    }\n",
    "\n",
    "    metric = {\n",
    "        'name': 'Run Loss, Validation',\n",
    "        'goal': 'minimize'\n",
    "    }\n",
    "\n",
    "    sweep_config['metric'] = metric\n",
    "\n",
    "    # Tuned hyperparameters\n",
    "    parameters_dict = {\n",
    "        'optimiser': {\n",
    "            'values': ['adam', 'sgd']\n",
    "        },\n",
    "        'learning_rate': {\n",
    "            # a flat distribution between 0 and 0.1\n",
    "            'distribution': 'uniform',\n",
    "            'min': 0.00001,\n",
    "            'max': 0.1\n",
    "        },\n",
    "        'batch_size': {\n",
    "            # Uniformly-distributed between 5-15\n",
    "            'distribution': 'int_uniform',\n",
    "            'min': 5,\n",
    "            'max': 25\n",
    "        },\n",
    "        'weight_decay': {\n",
    "            'distribution': 'uniform',\n",
    "            'min': 1e-8,\n",
    "            'max': 1e-2\n",
    "        },\n",
    "        'encoder_name': {\n",
    "            'values': [ 'resnet34', 'vgg19', 'dpn68']\n",
    "        },\n",
    "        \n",
    "    }\n",
    "    sweep_config['parameters'] = parameters_dict\n",
    "\n",
    "    # Fixed hyperparamters\n",
    "    parameters_dict.update({\n",
    "        'momentum': {\n",
    "            'value': 0.9},\n",
    "        'validation_percent': {\n",
    "            'value': 0.2},\n",
    "        'img_scale': {\n",
    "            'value': 0.5},\n",
    "        'epochs': {\n",
    "            'value': 10}\n",
    "    })\n",
    "\n",
    "    sweep_id = wandb.sweep(sweep_config, project=\"hyp-sweep-jasmin-mcl\")\n",
    "\n",
    "    n_tuning = 30\n",
    "    wandb.agent(sweep_id, function=train_and_validate, count=n_tuning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c29689fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
