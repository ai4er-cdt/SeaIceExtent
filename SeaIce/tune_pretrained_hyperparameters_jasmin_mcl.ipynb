{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d98a6d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This script runs best if you login to wandb through the terminal before running the script using: $wandb login\"\"\"\n",
    "\n",
    "#try:\n",
    "#    from dataset_preparation import *\n",
    "#    from evaluation import *\n",
    "#    from network_structure import *\n",
    "#except:\n",
    "from unet.dataset_preparation import *\n",
    "from unet.evaluation_mcl import *\n",
    "from unet.network_structure import *\n",
    "\n",
    "import wandb\n",
    "import torch.optim as optim\n",
    "from pathlib import Path\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def create_checkpoint_dir(parent_folder_checkpoint, img_type, model_type,\n",
    "                         encoder, optimiser, lr, batchsize, weightdecay):\n",
    "    now = datetime.now()\n",
    "    dt_string = now.strftime(\"%d%m%Y_%H%M%S\")\n",
    "    dir_list = os.listdir(parent_folder_checkpoint)\n",
    "    FOLDER_EXISTS = True\n",
    "    x = 1 \n",
    "    while FOLDER_EXISTS: \n",
    "        dir_checkpoint = str('{}_{}_encode{}_optm{}_batchsize{}_learnrate{}_weightdecay{}_date{}_{}'.format(model_type, img_type, \n",
    "                                                   str(encoder), str(optimiser), str(batchsize), str(lr), \n",
    "                                                   str(weightdecay), str(dt_string), str(x)))\n",
    "        path = os.path.join(parent_folder_checkpoint, Path(dir_checkpoint))\n",
    "        if dir_checkpoint in dir_list:\n",
    "            x += 1\n",
    "        else:\n",
    "            FOLDER_EXISTS = False\n",
    "            os.mkdir(path)\n",
    "            return(path)\n",
    "        \n",
    "\n",
    "def build_optimiser(network, config):\n",
    "    \"\"\"Builds the training optimiser based on the parametersd in the config file\"\"\"\n",
    "    if config.optimiser == \"sgd\":\n",
    "        optimiser = optim.SGD(network.parameters(),\n",
    "                              config.learning_rate, config.momentum)\n",
    "    elif config.optimiser == \"adam\":\n",
    "        optimiser = optim.Adam(network.parameters(),\n",
    "                               lr=config.learning_rate)\n",
    "    return optimiser\n",
    "\n",
    "\n",
    "def train_and_validate(config=None, amp=False, device=torch.device('cuda')):\n",
    "\n",
    "    # TEMP\n",
    "    momentum = 0.9\n",
    "    validation_percent = 0.2\n",
    "    img_scale = 0.5\n",
    "    epochs = 1\n",
    "    optimiser = 'adam'\n",
    "    learning_Rate = 0.004\n",
    "    batch_size = 15\n",
    "    weight_decay = 1e-4\n",
    "    encoder_name = 'resnet34'\n",
    "    # TEMP\n",
    "    \n",
    "    \n",
    "    # Inputs for the helper functions\n",
    "    # Image and model params\n",
    "    #img_dir = '/home/users/mcl66/SeaIce/tiled512/'\n",
    "    img_dir = '/mnt/g/Shared drives/2021-gtc-sea-ice/trainingdata/tiled512/train/'\n",
    "    image_type = 'sar'\n",
    "    model_type = \"pretrained\"\n",
    "    workers = 8\n",
    "    \n",
    "    # for checkpoint saving\n",
    "    checkpoint_dir = '/mnt/c/users/madel/Desktop/code/SeaIceExtent/checkpoints-mcl/'\n",
    "    save_checkpoint = True\n",
    "    print(checkpoint_dir)\n",
    "    \n",
    "    # unlikely to need modification\n",
    "    return_type = 'dict'\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    if image_type == \"sar\":\n",
    "        is_single_band = True\n",
    "        n_channels = 1\n",
    "    elif image_type == \"modis\":\n",
    "        is_single_band = False\n",
    "        n_channels = 3\n",
    "\n",
    "    # Initialize a new wandb run\n",
    "    #with A = 1 #wandb.init(config=config):\n",
    "        # If called by wandb.agent, as below,\n",
    "        # this config will be set by Sweep Controller\n",
    "        \n",
    "        #config = wandb.config\n",
    "        \n",
    "        \n",
    "        #net = smp.Unet(encoder_name=config.encoder_name, encoder_weights='imagenet', decoder_use_batchnorm=True,\n",
    "        #         decoder_attention_type=None, in_channels=n_channels, classes=1, encoder_depth=5)\n",
    "        net = smp.Unet(encoder_name=encoder_name, encoder_weights='imagenet', decoder_use_batchnorm=True,\n",
    "                 decoder_attention_type=None, in_channels=n_channels, classes=1, encoder_depth=5)\n",
    "        net = net.double()\n",
    "\n",
    "        # Loader\n",
    "        img_list = create_npy_list(img_dir, image_type)\n",
    "\n",
    "        #train_img_list, val_img_list, n_train, n_val = split_img_list(img_list, config.validation_percent)\n",
    "        train_img_list, val_img_list, n_train, n_val = split_img_list(img_list, validation_percent)\n",
    "\n",
    "        train_dataset = CustomImageAugmentDataset(train_img_list, is_single_band, return_type, True)\n",
    "        validation_dataset = CustomImageAugmentDataset(val_img_list, is_single_band, return_type, False)\n",
    "        print(train_dataset)\n",
    "\n",
    "        #train_loader, val_loader = create_dataloaders(train_dataset, validation_dataset, config.batch_size, workers)\n",
    "        train_loader, val_loader = create_dataloaders(train_dataset, validation_dataset, batch_size, workers)\n",
    "\n",
    "        # Optimiser\n",
    "        #optimiser = build_optimiser(net, config)\n",
    "        optimiser = optim.Adam(net.parameters(),\n",
    "                               lr=learning_rate)\n",
    "\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimiser, 'max',\n",
    "                                                         patience=2)  # CHECK goal: maximize Dice score\n",
    "        grad_scaler = torch.cuda.amp.GradScaler(enabled=False)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        global_step = 0\n",
    "        \n",
    "        loss = smp.utils.losses.DiceLoss()\n",
    "        metrics = [smp.utils.metrics.IoU(threshold=0.5),]\n",
    "        \n",
    "        train_epoch = smp.utils.train.TrainEpoch(\n",
    "            net,\n",
    "            loss=loss,\n",
    "            metrics=metrics,\n",
    "            optimizer=optimiser,\n",
    "            #verbose=True,\n",
    "            device=processor\n",
    "        )\n",
    "\n",
    "        valid_epoch = smp.utils.train.ValidEpoch(\n",
    "            net,\n",
    "            loss=loss,\n",
    "            metrics=metrics,\n",
    "            #verbose=True,\n",
    "            device=processor\n",
    "        )\n",
    "\n",
    "\n",
    "        # Begin training\n",
    "        run_loss_train = 0\n",
    "        run_loss_val = 0\n",
    "        dir_checkpoint = ''\n",
    "        #for epoch in range(config.epochs):\n",
    "        for epoch in range(epochs):\n",
    "            train_logs = train_epoch.run(train_loader)\n",
    "            valid_logs = valid_epoch.run(val_loader)\n",
    "\n",
    "            # do something (save model, change lr, etc.)\n",
    "            if max_score < valid_logs['iou_score']:\n",
    "                max_score = valid_logs['iou_score']\n",
    "                #dir_checkpoint = create_checkpoint_dir(folder_checkpoint, image_type, model_type, config.enconder_name, config.optimiser, \n",
    "                #                                           config.learning_rate, config.batch_size, config.weight_decay)\n",
    "                dir_checkpoint = create_checkpoint_dir(folder_checkpoint, image_type, model_type, enconder_name, optimiser, \n",
    "                                                           learning_rate, batch_size, weight_decay)\n",
    "                torch.save(net.state_dict(), str(dir_checkpoint / 'checkpoint_epoch{}.pth'.format(epoch + 1)))\n",
    "            \n",
    "            if i == n_epochs/2:\n",
    "                optimizer.param_groups[0]['lr'] = 1e-5\n",
    "            \n",
    "            # Optimisation\n",
    "            optimiser.zero_grad(set_to_none=True)\n",
    "            grad_scaler.scale(loss).backward()\n",
    "            grad_scaler.step(optimiser)\n",
    "            grad_scaler.update()\n",
    "\n",
    "            pbar.update(images.shape[0])\n",
    "            global_step += 1\n",
    "            batch_loss = loss.item()\n",
    "            epoch_loss += batch_loss\n",
    "            #wandb.log({\"Batch Loss, Training\": batch_loss}, step=global_step)\n",
    "            pbar.set_postfix(**{'loss (batch)': loss.item()})\n",
    "            n_batches += 1\n",
    "\n",
    "            #val_score = evaluate(net, val_loader, device, epsilon=config.weight_decay)\n",
    "            val_score = evaluate(net, val_loader, device, epsilon=weight_decay)\n",
    "            \n",
    "            print(f'\\nVal Score: {val_score}, Epoch: {epoch}')\n",
    "\n",
    "            #wandb.log({\"Batch Loss, Validation\": val_score_list[index]}, step=global_step-(len(val_score_list)+index))\n",
    "            #wandb.log({\"Epoch Validation Loss\": val_score}) , step=global_step-(len(val_score_list)+index)\n",
    "            scheduler.step(val_score)\n",
    "\n",
    "            avg_epoch_training_loss = epoch_loss / n_batches\n",
    "            #wandb.log({\"Epoch Training Loss\": avg_epoch_training_loss})\n",
    "            run_loss_train += avg_epoch_training_loss\n",
    "            #run_loss_val += val_score\n",
    "            print('Logging Epoch Scores')\n",
    "          #  wandb.log({\"Epoch Loss, Training\": avg_epoch_training_loss, \"Epoch Loss, Validation\": val_score, \"Epoch\": epoch+1}, step=global_step)\n",
    "          #  if save_checkpoint:\n",
    "         #       if epoch == 0:\n",
    "        #            dir_checkpoint = create_checkpoint_dir(folder_checkpoint, image_type, model_type, config.optimiser, \n",
    "        #                                                   config.learning_rate, config.batch_size, config.weight_decay)\n",
    "       #         torch.save(net.state_dict(), str(dir_checkpoint / 'checkpoint_epoch{}.pth'.format(epoch + 1)))\n",
    "      #  wandb.log({\"Run Loss, Training\": run_loss_train / config.epochs, \"Run Loss, Validation\": val_score}, step=global_step)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "\n",
    "#     #wandb.init(project=\"test-hyptuning\")\n",
    "\n",
    "#     # Configuring wandb settings\n",
    "#     sweep_config = {\n",
    "#         'method': 'random',  # Random search method -- less computationally expensive yet effective.\n",
    "#     }\n",
    "\n",
    "#     metric = {\n",
    "#         'name': 'Run Loss, Validation',\n",
    "#         'goal': 'minimize'\n",
    "#     }\n",
    "\n",
    "#     sweep_config['metric'] = metric\n",
    "\n",
    "#     # Tuned hyperparameters\n",
    "#     parameters_dict = {\n",
    "#         'optimiser': {\n",
    "#             'values': ['adam', 'sgd']\n",
    "#         },\n",
    "#         'learning_rate': {\n",
    "#             # a flat distribution between 0 and 0.1\n",
    "#             'distribution': 'uniform',\n",
    "#             'min': 0.00001,\n",
    "#             'max': 0.1\n",
    "#         },\n",
    "#         'batch_size': {\n",
    "#             # Uniformly-distributed between 5-15\n",
    "#             'distribution': 'int_uniform',\n",
    "#             'min': 5,\n",
    "#             'max': 25\n",
    "#         },\n",
    "#         'weight_decay': {\n",
    "#             'distribution': 'uniform',\n",
    "#             'min': 1e-8,\n",
    "#             'max': 1e-2\n",
    "#         },\n",
    "#         'encoder_name': {\n",
    "#             'values': ['resnet34', 'vgg19', 'dpn68']\n",
    "#         },\n",
    "        \n",
    "#     }\n",
    "#     sweep_config['parameters'] = parameters_dict\n",
    "\n",
    "#     # Fixed hyperparamters\n",
    "#     parameters_dict.update({\n",
    "#         'momentum': {\n",
    "#             'value': 0.9},\n",
    "#         'validation_percent': {\n",
    "#             'value': 0.2},\n",
    "#         'img_scale': {\n",
    "#             'value': 0.5},\n",
    "#         'epochs': {\n",
    "#             'value': 10}\n",
    "#     })\n",
    "\n",
    "#     sweep_id = wandb.sweep(sweep_config, project=\"hyp-sweep-jasmin-mcl\")\n",
    "\n",
    "#     n_tuning = 30\n",
    "#     wandb.agent(sweep_id, function=train_and_validate, count=n_tuning)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c29689fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_validate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
