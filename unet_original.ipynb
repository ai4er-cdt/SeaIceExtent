{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82618600",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" PLEASE DO NOT DIRECTLY EDIT THIS FILE.\n",
    "\n",
    "    This file contains a working version of the U-Net as described here: https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/\n",
    "    Many of the functions have been copied and slightly modified with comments from https://github.com/milesial/Pytorch-UNet\n",
    "    The new functions are for the Dataset class and for the create_npy_list function. \n",
    "    \n",
    "    The original U-Net file structure was split into multiple module files. For this working draft, \n",
    "    all functions have been incorporated into this single file in seperate cells along with their package dependancies\n",
    "    so that splitting into seperate modules in the future, if desired, will be simple.\n",
    "    \n",
    "    Additionally, this version has been slightly adapted for the single band SAR tile input compared to\n",
    "    the original 3 band optical image (RGB) input of U-NET. \n",
    "    \n",
    "    To adjust, must change: dir_image, dir_checkpoint, n_channels, n_classes and bilinear variables.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30dba784",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Parts of the U-Net model \"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => BatchNorm => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        # if you have padding issues, see\n",
    "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
    "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a2b640f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Full assembly of the parts to form the complete network \"\"\"\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(512, 1024 // factor)\n",
    "        self.up1 = Up(1024, 512 // factor, bilinear)\n",
    "        self.up2 = Up(512, 256 // factor, bilinear)\n",
    "        self.up3 = Up(256, 128 // factor, bilinear)\n",
    "        self.up4 = Up(128, 64, bilinear)\n",
    "        self.outc = OutConv(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0aaaab1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" CNN Dataset preparation functions \"\"\"\n",
    "\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset  # For custom data-sets\n",
    "from torchvision import transforms\n",
    "import glob\n",
    "import numpy as np\n",
    "# From https://discuss.pytorch.org/t/beginner-how-do-i-write-a-custom-dataset-that-allows-me-to-return-image-and-its-target-image-not-label-as-a-pair/13988/4\n",
    "# And https://discuss.pytorch.org/t/how-make-customised-dataset-for-semantic-segmentation/30881\n",
    "\n",
    "\n",
    "def create_npy_list(image_directory, img_string=\"sar\"):\n",
    "    \"\"\"A function that returns a list of the names of the SAR/MODIS and labelled .npy files in a directory. These lists can\n",
    "    then be used as an argument for the Dataset class instantiation. The function also checks that the specified directory \n",
    "    contains matching sar or MODIS/labelled pairs -- specifically, a label.npy file for each image file.\"\"\"\n",
    "\n",
    "    img_names = sorted(glob.glob(str(image_directory) + '/*_' + img_string + '.npy'))\n",
    "    label_names = sorted(glob.glob(str(image_directory) + '/*_labels.npy'))\n",
    "\n",
    "    # In-depth file-by-file check for matching sar-label pairs in the directory -- assuming  each sar image has a corresponding\n",
    "    # labeled image.\n",
    "    img_label_pairs = []\n",
    "    for image in img_names:\n",
    "        expected_label_name = image.replace(img_string, \"labels\")\n",
    "        if expected_label_name in label_names:\n",
    "            img_label_pairs.append((image, expected_label_name))\n",
    "        else:\n",
    "            raise Exception(f'{img_string} tile name {image} does not have a matching labeled tile.')\n",
    "            \n",
    "    return img_label_pairs\n",
    "\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    \"\"\"GTC Code for a dataset class. The class is instantiated with list of filenames within a directory (created using\n",
    "    the list_npy_filenames function). The __getitem__ method pairs up corresponding sar-label .npy file pairs. This\n",
    "    dataset can then be input to a dataloader.\"\"\"\n",
    "    \n",
    "    def __init__(self, paths, isSingleBand = True):\n",
    "        self.paths = paths\n",
    "        self.isSingleBand = isSingleBand\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image = torch.from_numpy(np.vstack(np.load(self.paths[index][0])).astype(float))\n",
    "        if self.isSingleBand:\n",
    "            image = image[None,:]\n",
    "        else:\n",
    "            image = torch.permute(image, (2,0,1))\n",
    "        mask_raw = (np.load(self.paths[index][1]))\n",
    "        maskremap100 = np.where(mask_raw == 100, 0, mask_raw)\n",
    "        maskremap200 = np.where(maskremap100 == 200, 1, maskremap100)\n",
    "        mask = torch.from_numpy(np.vstack(maskremap200).astype(float))\n",
    "        \n",
    "        #assert image.size == mask.size, \\\n",
    "        #    'Image and mask {name} should be the same size, but are {img.size} and {mask.size}'\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'mask': mask\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e607a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Dice Loss scoring functions \"\"\"\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "def dice_coeff(input: Tensor, target: Tensor, reduce_batch_first: bool = False, epsilon=1e-6):\n",
    "    # Average of Dice coefficient for all batches, or for a single mask\n",
    "    assert input.size() == target.size()\n",
    "    if input.dim() == 2 and reduce_batch_first:\n",
    "        raise ValueError(f'Dice: asked to reduce batch but got tensor without batch dimension (shape {input.shape})')\n",
    "\n",
    "    if input.dim() == 2 or reduce_batch_first:\n",
    "        inter = torch.dot(input.reshape(-1), target.reshape(-1))\n",
    "        sets_sum = torch.sum(input) + torch.sum(target)\n",
    "        if sets_sum.item() == 0:\n",
    "            sets_sum = 2 * inter\n",
    "\n",
    "        return (2 * inter + epsilon) / (sets_sum + epsilon)\n",
    "    else:\n",
    "        # compute and average metric for each batch element\n",
    "        dice = 0\n",
    "        for i in range(input.shape[0]):\n",
    "            dice += dice_coeff(input[i, ...], target[i, ...])\n",
    "        return dice / input.shape[0]\n",
    "\n",
    "\n",
    "def multiclass_dice_coeff(input: Tensor, target: Tensor, reduce_batch_first: bool = False, epsilon=1e-6):\n",
    "    # Average of Dice coefficient for all classes\n",
    "    assert input.size() == target.size()\n",
    "    dice = 0\n",
    "    for channel in range(input.shape[1]):\n",
    "        dice += dice_coeff(input[:, channel, ...], target[:, channel, ...], reduce_batch_first, epsilon)\n",
    "\n",
    "    return dice / input.shape[1]\n",
    "\n",
    "\n",
    "def dice_loss(input: Tensor, target: Tensor, multiclass: bool = False):\n",
    "    # Dice loss (objective to minimize) between 0 and 1\n",
    "    assert input.size() == target.size()\n",
    "    fn = multiclass_dice_coeff if multiclass else dice_coeff\n",
    "    return 1 - fn(input, target, reduce_batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "162a449f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Model evaluation functions \"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def evaluate(net, dataloader, device):\n",
    "    net.eval()\n",
    "    num_val_batches = len(dataloader)\n",
    "    dice_score = 0\n",
    "\n",
    "    # iterate over the validation set\n",
    "    for batch in tqdm(dataloader, total=num_val_batches, desc='Validation round', unit='batch', leave=False):\n",
    "        image, mask_true = batch['image'], batch['mask']\n",
    "        # move images and labels to correct device and type\n",
    "        image = image.to(device=device, dtype=torch.float32)\n",
    "        mask_true = mask_true.to(device=device, dtype=torch.long)\n",
    "        mask_true = F.one_hot(mask_true, net.n_classes).permute(0, 3, 1, 2).float()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # predict the mask\n",
    "            mask_pred = net(image)\n",
    "\n",
    "            # convert to one-hot format\n",
    "            if net.n_classes == 1:\n",
    "                mask_pred = (F.sigmoid(mask_pred) > 0.5).float()\n",
    "                # compute the Dice score\n",
    "                dice_score += dice_coeff(mask_pred, mask_true, reduce_batch_first=False)\n",
    "            else:\n",
    "                mask_pred = F.one_hot(mask_pred.argmax(dim=1), net.n_classes).permute(0, 3, 1, 2).float()\n",
    "                # compute the Dice score, ignoring background\n",
    "                dice_score += multiclass_dice_coeff(mask_pred[:, 1:, ...], mask_true[:, 1:, ...], reduce_batch_first=False)\n",
    "\n",
    "           \n",
    "\n",
    "    net.train()\n",
    "\n",
    "    # Fixes a potential division by zero error\n",
    "    if num_val_batches == 0:\n",
    "        return dice_score\n",
    "    return dice_score / num_val_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba8f3430",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Prediction plotting function \"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_img_and_mask(img, mask):\n",
    "    classes = mask.shape[0] if len(mask.shape) > 2 else 1\n",
    "    fig, ax = plt.subplots(1, classes + 1)\n",
    "    ax[0].set_title('Input image')\n",
    "    ax[0].imshow(img)\n",
    "    if classes > 1:\n",
    "        for i in range(classes):\n",
    "            ax[i + 1].set_title(f'Output mask (class {i + 1})')\n",
    "            ax[i + 1].imshow(mask[:, :, i])\n",
    "    else:\n",
    "        ax[1].set_title(f'Output mask')\n",
    "        ax[1].imshow(mask)\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfefa63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using device cpu\n",
      "INFO: Network:\n",
      "\t1 input channels\n",
      "\t2 output channels (classes)\n",
      "\tBilinear upscaling\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33manony-mouse-194647\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.10 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/anony-mouse-194647/U-Net/runs/2m9v7sfm?apiKey=792f0a7aa6f2f18802a7f66fe7cc895decf8c478\" target=\"_blank\">classic-spaceship-64</a></strong> to <a href=\"https://wandb.ai/anony-mouse-194647/U-Net?apiKey=792f0a7aa6f2f18802a7f66fe7cc895decf8c478\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Starting training:\n",
      "        Epochs:          5\n",
      "        Batch size:      1\n",
      "        Learning rate:   1e-05\n",
      "        Training size:   1669\n",
      "        Validation size: 185\n",
      "        Checkpoints:     True\n",
      "        Device:          cpu\n",
      "        Images scaling:  0.5\n",
      "        Mixed Precision: False\n",
      "    \n",
      "Epoch 1/5:   0%|                                                                              | 0/1669 [00:00<?, ?img/s]/home/mlisaius/miniconda3/envs/unetenv/lib/python3.7/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "Epoch 1/5:   1%|â–Ž                                                | 9/1669 [01:03<3:14:30,  7.03s/img, loss (batch)=1.28]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mlisaius/miniconda3/envs/unetenv/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/mlisaius/miniconda3/envs/unetenv/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/mlisaius/miniconda3/envs/unetenv/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/mlisaius/miniconda3/envs/unetenv/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mlisaius/miniconda3/envs/unetenv/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/mlisaius/miniconda3/envs/unetenv/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/mlisaius/miniconda3/envs/unetenv/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/mlisaius/miniconda3/envs/unetenv/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mlisaius/miniconda3/envs/unetenv/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/mlisaius/miniconda3/envs/unetenv/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/mlisaius/miniconda3/envs/unetenv/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/mlisaius/miniconda3/envs/unetenv/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mlisaius/miniconda3/envs/unetenv/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/mlisaius/miniconda3/envs/unetenv/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/mlisaius/miniconda3/envs/unetenv/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/mlisaius/miniconda3/envs/unetenv/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "INFO: Saved interrupt\n",
      "ERROR: Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "INFO: \n",
      "Unfortunately, your original traceback can not be constructed.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_25353/564362544.py\", line 188, in <module>\n",
      "    amp=args.amp)\n",
      "  File \"/tmp/ipykernel_25353/564362544.py\", line 88, in train_net\n",
      "    masks_pred = net(images)\n",
      "  File \"/home/mlisaius/miniconda3/envs/unetenv/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/tmp/ipykernel_25353/3967244499.py\", line 31, in forward\n",
      "    x = self.up4(x, x1)\n",
      "  File \"/home/mlisaius/miniconda3/envs/unetenv/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/tmp/ipykernel_25353/2978483942.py\", line 68, in forward\n",
      "    return self.conv(x)\n",
      "  File \"/home/mlisaius/miniconda3/envs/unetenv/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/tmp/ipykernel_25353/2978483942.py\", line 25, in forward\n",
      "    return self.double_conv(x)\n",
      "  File \"/home/mlisaius/miniconda3/envs/unetenv/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/mlisaius/miniconda3/envs/unetenv/lib/python3.7/site-packages/torch/nn/modules/container.py\", line 141, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/mlisaius/miniconda3/envs/unetenv/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/mlisaius/miniconda3/envs/unetenv/lib/python3.7/site-packages/torch/nn/modules/batchnorm.py\", line 179, in forward\n",
      "    self.eps,\n",
      "  File \"/home/mlisaius/miniconda3/envs/unetenv/lib/python3.7/site-packages/torch/nn/functional.py\", line 2283, in batch_norm\n",
      "    input, weight, bias, running_mean, running_var, training, momentum, eps, torch.backends.cudnn.enabled\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mlisaius/miniconda3/envs/unetenv/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3457, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_25353/564362544.py\", line 192, in <module>\n",
      "    sys.exit(0)\n",
      "  File \"/home/mlisaius/miniconda3/envs/unetenv/lib/python3.7/site-packages/wandb/sdk/lib/exit_hooks.py\", line 38, in exit\n",
      "    self._orig_exit(orig_code)\n",
      "SystemExit: 0\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mlisaius/miniconda3/envs/unetenv/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/mlisaius/miniconda3/envs/unetenv/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/mlisaius/miniconda3/envs/unetenv/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/mlisaius/miniconda3/envs/unetenv/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "AttributeError: 'tuple' object has no attribute 'tb_frame'\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_25353/564362544.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    187\u001b[0m                   \u001b[0mval_percent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m                   amp=args.amp)\n\u001b[0m\u001b[1;32m    189\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_25353/564362544.py\u001b[0m in \u001b[0;36mtrain_net\u001b[0;34m(net, device, epochs, batch_size, learning_rate, val_percent, save_checkpoint, img_scale, amp)\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0mmasks_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasks_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/unetenv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_25353/3967244499.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/unetenv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_25353/2978483942.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x1, x2)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/unetenv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_25353/2978483942.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/unetenv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/unetenv/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/unetenv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/unetenv/lib/python3.7/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mexponential_average_factor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         )\n",
      "\u001b[0;32m~/miniconda3/envs/unetenv/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2282\u001b[0m     return torch.batch_norm(\n\u001b[0;32m-> 2283\u001b[0;31m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2284\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_25353/564362544.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Saved interrupt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/unetenv/lib/python3.7/site-packages/wandb/sdk/lib/exit_hooks.py\u001b[0m in \u001b[0;36mexit\u001b[0;34m(self, code)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_orig_exit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSystemExit\u001b[0m: 0",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/unetenv/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2069\u001b[0m                            'the full traceback.\\n']\n\u001b[1;32m   2070\u001b[0m                     stb.extend(self.InteractiveTB.get_exception_only(etype,\n\u001b[0;32m-> 2071\u001b[0;31m                                                                      value))\n\u001b[0m\u001b[1;32m   2072\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2073\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/unetenv/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mget_exception_only\u001b[0;34m(self, etype, value)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mexception\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \"\"\"\n\u001b[0;32m--> 754\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mListTB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstructured_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshow_exception_only\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/unetenv/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, context)\u001b[0m\n\u001b[1;32m    631\u001b[0m                     chained_exceptions_tb_offset, context)\n\u001b[1;32m    632\u001b[0m                 \u001b[0;34m+\u001b[0m \u001b[0mchained_exception_message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m                 + out_list)\n\u001b[0m\u001b[1;32m    634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/unetenv/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m         return FormattedTB.structured_traceback(\n\u001b[0;32m-> 1368\u001b[0;31m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0m\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/unetenv/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1267\u001b[0m             return VerboseTB.structured_traceback(\n\u001b[0;32m-> 1268\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1269\u001b[0m             )\n\u001b[1;32m   1270\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Minimal'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/unetenv/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0;32m-> 1125\u001b[0;31m                                                                tb_offset)\n\u001b[0m\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m         \u001b[0mcolors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mColors\u001b[0m  \u001b[0;31m# just a shorthand + quicker name lookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/unetenv/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/unetenv/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "\"\"\" Implementation of model \"\"\"\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "dir_img = Path('/mnt/g/Shared drives/2021-gtc-sea-ice/trainingdata/tiled/')\n",
    "dir_checkpoint = Path('/mnt/g/Shared drives/2021-gtc-sea-ice/model/checkpoints/unet_orig')\n",
    "\n",
    "\n",
    "def train_net(net,\n",
    "              device,\n",
    "              epochs: int = 1,\n",
    "              batch_size: int = 10,\n",
    "              learning_rate: float = 0.001,\n",
    "              val_percent: float = 0.1,\n",
    "              save_checkpoint: bool = True,\n",
    "              img_scale: float = 0.5,\n",
    "              amp: bool = False):\n",
    "    # 1. Create dataset\n",
    "    img_list = create_npy_list(dir_img)\n",
    "    dataset = CustomImageDataset(img_list, True)\n",
    "    \n",
    "    # 2. Split into train / validation partitions\n",
    "    n_val = int(len(dataset) * val_percent)\n",
    "    n_train = len(dataset) - n_val\n",
    "    train_set, val_set = random_split(dataset, [n_train, n_val], generator=torch.Generator().manual_seed(0))\n",
    "    \n",
    "    # 3. Create data loaders\n",
    "    loader_args = dict(batch_size=batch_size, num_workers=4, pin_memory=True)\n",
    "    train_loader = DataLoader(train_set, shuffle=True, **loader_args)\n",
    "    val_loader = DataLoader(val_set, shuffle=False, drop_last=True, **loader_args)\n",
    "    \n",
    "    # Initialize logging\n",
    "    experiment = wandb.init(project='U-Net', resume='allow', anonymous='must')\n",
    "    experiment.config.update(dict(epochs=epochs, batch_size=batch_size, learning_rate=learning_rate,\n",
    "                                  val_percent=val_percent, save_checkpoint=save_checkpoint, img_scale=img_scale,\n",
    "                                  amp=amp))\n",
    "\n",
    "    logging.info(f'''Starting training:\n",
    "        Epochs:          {epochs}\n",
    "        Batch size:      {batch_size}\n",
    "        Learning rate:   {learning_rate}\n",
    "        Training size:   {n_train}\n",
    "        Validation size: {n_val}\n",
    "        Checkpoints:     {save_checkpoint}\n",
    "        Device:          {device.type}\n",
    "        Images scaling:  {img_scale}\n",
    "        Mixed Precision: {amp}\n",
    "    ''')\n",
    "\n",
    "    # 4. Set up the optimizer, the loss, the learning rate scheduler and the loss scaling for AMP\n",
    "    optimizer = optim.RMSprop(net.parameters(), lr=learning_rate, weight_decay=1e-8, momentum=0.9)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=2)  # goal: maximize Dice score\n",
    "    grad_scaler = torch.cuda.amp.GradScaler(enabled=amp)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    global_step = 0\n",
    "    \n",
    "    # 5. Begin training\n",
    "    for epoch in range(epochs):\n",
    "        net.train()\n",
    "        epoch_loss = 0\n",
    "        with tqdm(total=n_train, desc=f'Epoch {epoch + 1}/{epochs}', unit='img') as pbar:\n",
    "            for batch in train_loader:\n",
    "                images = batch['image']\n",
    "                true_masks = batch['mask']\n",
    "                \n",
    "                assert images.shape[1] == net.n_channels, \\\n",
    "                    f'Network has been defined with {net.n_channels} input channels, ' \\\n",
    "                    f'but loaded images have {images.shape[1]} channels of {images.shape}. Please check that ' \\\n",
    "                    'the images are loaded correctly.'\n",
    "\n",
    "                images = images.to(device=device, dtype=torch.float32)\n",
    "                true_masks = true_masks.to(device=device, dtype=torch.long)\n",
    "\n",
    "                with torch.cuda.amp.autocast(enabled=amp):\n",
    "                    masks_pred = net(images)\n",
    "                    loss = criterion(masks_pred, true_masks) \\\n",
    "                           + dice_loss(F.softmax(masks_pred, dim=1).float(),\n",
    "                                       F.one_hot(true_masks, net.n_classes).permute(0, 3, 1, 2).float(),\n",
    "                                       multiclass=True)\n",
    "                # change number for permute?\n",
    "                \n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                grad_scaler.scale(loss).backward()\n",
    "                grad_scaler.step(optimizer)\n",
    "                grad_scaler.update()\n",
    "\n",
    "                pbar.update(images.shape[0])\n",
    "                global_step += 1\n",
    "                epoch_loss += loss.item()\n",
    "                experiment.log({\n",
    "                    'train loss': loss.item(),\n",
    "                    'step': global_step,\n",
    "                    'epoch': epoch\n",
    "                })\n",
    "                pbar.set_postfix(**{'loss (batch)': loss.item()})\n",
    "\n",
    "                # Evaluation round\n",
    "                division_step = (n_train // (10 * batch_size))\n",
    "                if division_step > 0:\n",
    "                    if global_step % division_step == 0:\n",
    "                        histograms = {}\n",
    "                        for tag, value in net.named_parameters():\n",
    "                            tag = tag.replace('/', '.')\n",
    "                            histograms['Weights/' + tag] = wandb.Histogram(value.data.cpu())\n",
    "                            histograms['Gradients/' + tag] = wandb.Histogram(value.grad.data.cpu())\n",
    "\n",
    "                        val_score = evaluate(net, val_loader, device)\n",
    "                        scheduler.step(val_score)\n",
    "\n",
    "                        logging.info('Validation Dice score: {}'.format(val_score))\n",
    "                        experiment.log({\n",
    "                            'learning rate': optimizer.param_groups[0]['lr'],\n",
    "                            'validation Dice': val_score,\n",
    "                            'images': wandb.Image(images[0].cpu()),\n",
    "                            'masks': {\n",
    "                                'true': wandb.Image(true_masks[0].float().cpu()),\n",
    "                                'pred': wandb.Image(torch.softmax(masks_pred, dim=1).argmax(dim=1)[0].float().cpu()),\n",
    "                            },\n",
    "                            'step': global_step,\n",
    "                            'epoch': epoch,\n",
    "                            **histograms\n",
    "                        })\n",
    "\n",
    "        if save_checkpoint:\n",
    "            Path(dir_checkpoint).mkdir(parents=True, exist_ok=True)\n",
    "            torch.save(net.state_dict(), str(dir_checkpoint / 'checkpoint_epoch{}.pth'.format(epoch + 1)))\n",
    "            logging.info(f'Checkpoint {epoch + 1} saved!')\n",
    "\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(description='Train the UNet on images and target masks')\n",
    "    parser.add_argument('--epochs', '-e', metavar='E', type=int, default=5, help='Number of epochs')\n",
    "    parser.add_argument('--batch-size', '-b', dest='batch_size', metavar='B', type=int, default=1, help='Batch size')\n",
    "    parser.add_argument('--learning-rate', '-l', metavar='LR', type=float, default=0.00001,\n",
    "                        help='Learning rate', dest='lr')\n",
    "    parser.add_argument('--load', '-f', type=str, default=False, help='Load model from a .pth file')\n",
    "    parser.add_argument('--scale', '-s', type=float, default=0.5, help='Downscaling factor of the images')\n",
    "    parser.add_argument('--validation', '-v', dest='val', type=float, default=10.0,\n",
    "                        help='Percent of the data that is used as validation (0-100)')\n",
    "    parser.add_argument('--amp', action='store_true', default=False, help='Use mixed precision')\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = get_args()\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    logging.info(f'Using device {device}')\n",
    "\n",
    "    # Change here to adapt to your data\n",
    "    # n_channels=3 for RGB images\n",
    "    # n_classes is the number of probabilities you want to get per pixel\n",
    "    net = UNet(n_channels=1, n_classes=2, bilinear=True)\n",
    "\n",
    "    logging.info(f'Network:\\n'\n",
    "                 f'\\t{net.n_channels} input channels\\n'\n",
    "                 f'\\t{net.n_classes} output channels (classes)\\n'\n",
    "                 f'\\t{\"Bilinear\" if net.bilinear else \"Transposed conv\"} upscaling')\n",
    "\n",
    "#    if args.load:\n",
    "#        net.load_state_dict(torch.load(args.load, map_location=device))\n",
    "#        logging.info(f'Model loaded from {args.load}')\n",
    "\n",
    "    net.to(device=device)\n",
    "    try:\n",
    "        train_net(net=net,\n",
    "                  epochs=args.epochs,\n",
    "                  batch_size=args.batch_size,\n",
    "                  learning_rate=args.lr,\n",
    "                  device=device,\n",
    "                  img_scale=args.scale,\n",
    "                  val_percent=args.val / 100,\n",
    "                  amp=args.amp)\n",
    "    except KeyboardInterrupt:\n",
    "        torch.save(net.state_dict(), 'INTERRUPTED.pth')\n",
    "        logging.info('Saved interrupt')\n",
    "        sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc43e7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#from unet import UNet as _UNet\n",
    "\n",
    "def unet_pretrained(pretrained=False):\n",
    "    \"\"\"\n",
    "    UNet model trained on the Carvana dataset ( https://www.kaggle.com/c/carvana-image-masking-challenge/data ).\n",
    "    Set the scale to 0.5 (50%) when predicting.\n",
    "    \"\"\"\n",
    "    net = UNet(n_channels=1, n_classes=2, bilinear=True)\n",
    "    if pretrained:\n",
    "        #checkpoint = # input checkpoint dir 'https://github.com/milesial/Pytorch-UNet/releases/download/v2.0/unet_carvana_scale0.5_epoch1.pth'\n",
    "        checkpoint = str(dir_checkpoint) + \"/checkpoint_epoch1.pth\" \n",
    "        net.load_state_dict(torch.load(checkpoint, map_location=device))\n",
    "        #net.load_state_dict(torch.hub.load(checkpoint, 'checkpoint_epoch1'))\n",
    "        # Use when loading from a git repository\n",
    "        #net.load_state_dict(torch.hub.load_state_dict_from_url(checkpoint, progress=True))\n",
    "\n",
    "    return net\n",
    "\n",
    "#test_net = unet_pretrained(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9f537c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb599530",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "#from utils.data_loading import BasicDataset\n",
    "#from unet import UNet\n",
    "#from utils.utils import plot_img_and_mask\n",
    "\n",
    "def predict_img(net,\n",
    "                full_img,\n",
    "                device,\n",
    "                scale_factor=1,\n",
    "                out_threshold=0.5):\n",
    "    net.eval()\n",
    "    img = full_img #torch.from_numpy(BasicDataset.preprocess(full_img, scale_factor, is_mask=False))\n",
    "    img = img.unsqueeze(0)\n",
    "    img = img.to(device=device, dtype=torch.float32)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = net(img)\n",
    "\n",
    "        if net.n_classes > 1:\n",
    "            probs = F.softmax(output, dim=1)[0]\n",
    "        else:\n",
    "            probs = torch.sigmoid(output)[0]\n",
    "\n",
    "        tf = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            #transforms.Resize((full_img.size[1], full_img.size[0])),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "        full_mask = tf(probs.cpu()).squeeze()\n",
    "\n",
    "    if net.n_classes == 1:\n",
    "        return (full_mask > out_threshold).numpy()\n",
    "    else:\n",
    "        return F.one_hot(full_mask.argmax(dim=0), net.n_classes).permute(2, 0, 1).numpy()\n",
    "\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(description='Predict masks from input images')\n",
    "    parser.add_argument('--model', '-m', default='MODEL.pth', metavar='FILE',\n",
    "                        help='Specify the file in which the model is stored')\n",
    "    parser.add_argument('--input', '-i', metavar='INPUT', nargs='+', help='Filenames of input images', required=True)\n",
    "    parser.add_argument('--output', '-o', metavar='INPUT', nargs='+', help='Filenames of output images')\n",
    "    parser.add_argument('--viz', '-v', action='store_true',\n",
    "                        help='Visualize the images as they are processed')\n",
    "    parser.add_argument('--no-save', '-n', action='store_true', help='Do not save the output masks')\n",
    "    parser.add_argument('--mask-threshold', '-t', type=float, default=0.5,\n",
    "                        help='Minimum probability value to consider a mask pixel white')\n",
    "    parser.add_argument('--scale', '-s', type=float, default=0.5,\n",
    "                        help='Scale factor for the input images')\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def get_output_filenames(args):\n",
    "    def _generate_name(fn):\n",
    "        split = os.path.splitext(fn)\n",
    "        return f'{split[0]}_OUT{split[1]}'\n",
    "\n",
    "    return args.output or list(map(_generate_name, args.input))\n",
    "\n",
    "\n",
    "def mask_to_image(mask: np.ndarray):\n",
    "    if mask.ndim == 2:\n",
    "        return Image.fromarray((mask * 255).astype(np.uint8))\n",
    "    elif mask.ndim == 3:\n",
    "        return Image.fromarray((np.argmax(mask, axis=0) * 255 / mask.shape[0]).astype(np.uint8))\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     args = get_args()\n",
    "#     in_files = args.input\n",
    "#     out_files = get_output_filenames(args)\n",
    "\n",
    "#     # update n_channels\n",
    "#     net = UNet(n_channels=1, n_classes=2)\n",
    "\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     logging.info(f'Loading model {args.model}')\n",
    "#     logging.info(f'Using device {device}')\n",
    "\n",
    "#     net.to(device=device)\n",
    "#     net.load_state_dict(torch.load(args.model, map_location=device))\n",
    "\n",
    "#     logging.info('Model loaded!')\n",
    "\n",
    "#     for i, filename in enumerate(in_files):\n",
    "#         logging.info(f'\\nPredicting image {filename} ...')\n",
    "#         img = Image.open(filename)\n",
    "\n",
    "#         mask = predict_img(net=net,\n",
    "#                            full_img=img,\n",
    "#                            scale_factor=args.scale,\n",
    "#                            out_threshold=args.mask_threshold,\n",
    "#                            device=device)\n",
    "\n",
    "#         if not args.no_save:\n",
    "#             out_filename = out_files[i]\n",
    "#             result = mask_to_image(mask)\n",
    "#             result.save(out_filename)\n",
    "#             logging.info(f'Mask saved to {out_filename}')\n",
    "\n",
    "#         if args.viz:\n",
    "#             logging.info(f'Visualizing results for image {filename}, close to continue...')\n",
    "#             plot_img_and_mask(img, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "16797182",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: '/mnt/g/Shared drives/2021-gtc-sea-ice/trainingdata/tiled/2011-01-24_134238_tile336_sar.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_25353/3897820098.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/mnt/g/Shared drives/2021-gtc-sea-ice/trainingdata/tiled/2011-01-24_134238_tile337_sar.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mthis_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/mnt/g/Shared drives/2021-gtc-sea-ice/trainingdata/tiled/2011-01-24_134238_tile336_sar.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/unetenv/lib/python3.7/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    415\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 22] Invalid argument: '/mnt/g/Shared drives/2021-gtc-sea-ice/trainingdata/tiled/2011-01-24_134238_tile336_sar.npy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "path = Path(\"/mnt/g/Shared drives/2021-gtc-sea-ice/trainingdata/tiled/2011-01-24_134238_tile337_sar.npy\")\n",
    "this_path = Path('/mnt/g/Shared drives/2021-gtc-sea-ice/trainingdata/tiled/2011-01-24_134238_tile336_sar.npy')\n",
    "image = torch.from_numpy(np.vstack(np.load(this_path)).astype(float))[None,:]\n",
    "\n",
    "net = UNet(n_channels=1, n_classes=2)\n",
    "\n",
    "checkpoint = str(dir_checkpoint) + \"/checkpoint_epoch1.pth\"\n",
    "net.load_state_dict(torch.load(checkpoint))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d8363ed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "prediction = predict_img(net=net,\n",
    "                            full_img=image,\n",
    "                            #scale_factor=args.scale,\n",
    "                            #out_threshold=args.mask_threshold,\n",
    "                            device=device)\n",
    "\n",
    "this_img = image.numpy().squeeze()\n",
    "this_img.shape\n",
    "\n",
    "\n",
    "plot_img_and_mask(this_img, prediction)\n",
    "\n",
    "#plt.plot(prediction[1])\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7fa2c6fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
